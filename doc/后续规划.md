# 后续规划

## 第一阶段

### 内容

制定下面的规划的目的是为了让该组件可扩展性更强，同时代码实现上更加优雅。

1、针对`L1`提供扩展点，支持根据需要扩展不同的`L1`实现。

2、针对`L2`提供扩展点，支持根据需要扩展不同的`L2`实现。

3、针对`缓存变更通知`提供扩展点，支持通过kafka、rocketmq等其他消息中间件来进行`缓存变更通知`。

### 状态
`2020-07-09 已完成`


---
## 第二阶段

### 内容
1、在高并发场景下的实战

2、只用二级缓存Redis的实战

3、一二级缓存结合使用的实战

### 状态
`2020-10-09 已完成`

---
## 第三阶段

### 内容
1、支持批量getOrLoad
> 实际项目中存在大量循环获取单个缓存的场景，多次交互性能受损，业务代码不够优雅，故提供批量获取功能更好的服务业务

2、支持批量put


### 状态
`2021-03-25 已完成`

---
## 第四阶段

### 内容
1、热key的自动识别功能
> 现状：目前每次大促都是通过人为配置相关热key到本地缓存。该方式需要人工介入，且容易出现误操作。
> 
> 目标：通过自动识别热key，来判断是否走本地缓存，无需人为配置热key到本地缓存，一方面可释放人力，另一方面可预防突发流量打垮下游服务。
> 
> 热key探测方案：
> - 自研（第二个版本的规划）
> - 京东hotkey（第一个版本采用该方式）
> - 阿里sentinel的热点参数限流


### 状态
`2021-06-10 已完成 - 京东hotkey 的接入`


---
## 第五阶段

### 内容
1、定义KeyBuilder来统一定义缓存key的构建，提升系统的扩展性

### 状态
`20230928 已完成`

### 原因
在CacheService.buildCacheKey(...)中，即可实现自定义缓存key的扩展性，也就是说，缓存key的构建，完全由使用者自己去定义，因此无需再多定义一个KeyBuilder模块。
需要注意的是，一级缓存和二级缓存的缓存key，有一点区别，一级缓存的cacheKey的前缀不含cacheName，二级缓存的cacheKey的前缀包含cacheName。


---
## 第六阶段

### 内容
1、支持只使用二级缓存Redis时给不同维度的缓存设置不同的过期时间
> 进度：已完成

### 状态
`20231122 已完成`

---
## 第七阶段

### 内容
1、支持批量evict缓存

2、支持二级缓存的缓存名称维度的clear

> 应小伙伴的要求，增加批量清理缓存的功能。

### 状态
`20231129 已完成`


---
## 第八阶段
### 内容
1、支持不同维度的缓存配置各自的缓存类型
> 目前缓存类型的配置维度太大，导致一个应用只能配置一个缓存类型。
>
> 进度：20241016 已完成

2、一个服务多redis实例的场景支持
> 应小伙伴的要求，增加批量清理缓存的功能。
> 
> 进度：20241203已完成
>


---
## 第九阶段
### 内容
1、自定义缓存过期策略，实现多节点L1缓存过期时间的统一
> 问题分析：
> - 目前存在分布式缓存一致性问题，同一个key在不同节点的一级缓存不一致的问题
> 
> 场景描述：
> - 节点1、2、3同时缓存同一个key> 
> - 节点1的缓存过期，节点2和3的缓存未过期 
> - 场景1：请求到节点1时触发过期事件 
> - 场景2：请求到节点2或3时获取到旧数据
>
> 核心问题：
> - 不同节点的一级缓存（Caffeine）过期时间不一致，导致某些节点持有过期数据而不自知
> 
> 解决方案：统一过期时间
> 
> 思路：
> - 让所有节点的一级缓存过期时间与Redis保持一致
> - 当某个节点检测到缓存过期时，主动通知其他节点清除本地缓存
> 
> 实现：
> - 自定义Caffeine的Expiry接口
> - 在加载数据时从Redis获取TTL，设置相同的过期时间
> - 使用refreshAfterWrite而不是expireAfterWrite来避免并发问题
> - 在一级缓存的过期监听器中发送过期通知消息（其他节点收到消息后清除对应的本地缓存）
> 
> 总结：
> - 在现有架构基础上，最大程度地保证缓存一致性，同时避免过度复杂的实现。
> 
> 进度：20250923 已完成
> 

---
## 第九阶段（重点）

### 内容
1、缓存的可视化管理+监控（有监控才有方向）

未来的方向：始于架构，精于治理。治理不仅是架构的延续，更是下一代应用中间件技术的演进方向。

> 现状：目前对于应用中的缓存现状是未知的，无法分析本地缓存的命中率，哪些key存在缓存穿透（次数？），哪些缓存走了本地缓存，缓存数量等等。
> 
> 目标：通过一个dashboard面板将缓存可视化，便于管控。（结合Prometheus监控体系+Grafana监控数据展示 ）

- 穿透次数
- 命中率排行榜
- 热key排行榜
- 大key排行榜
- 缓存服务器列表
- 缓存维度列表（基本信息）
- 缓存总数
- 缓存大小（占用内存）
- 缓存维度下缓存数量
- 缓存更新时间
- 缓存QPS
- 不同缓存服务器上缓存的一致性检测
- 服务器内存统计数据

l2cache-server

l2cache-dashboard

参考：https://ix47mvyf3j.feishu.cn/docx/TAyRdtWqnoHykUxbjb7cgTyZnlg

### 状态
规划中

---
## 第十阶段
1、缓存操作的异常时的处理和告警
- 缓存操作异常时的告警
- 缓存操作异常时的重试机制

### 状态
规划中

---
## 第十一阶段
完整的缓存更新解决方案

1、基于binlog模式的缓存数据更新
- 通过MQ的消费分组来订阅变更消息，实现不同服务的缓存更新
- 实现通用的解析，简化业务开发

2、基于Cache-Aside Pattern的缓存数据更新
- 双删模式
- 双写模式
- 统一抽象的实现，简化业务开发
- 注：单个缓存维度的双删/双写好处理，若一个业务操作，会操作多个缓存维度，这种情况怎么控制呢？
- Cache.put()的底层实现上，做一层包装，更新缓存时，根据指定策略进行淘汰缓存


### 状态
规划中


## 第十二阶段
1、新增Dragonfly二级缓存的支持

2、新增国产Solon框架的支持

### 状态
规划中